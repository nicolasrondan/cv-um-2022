{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F56hbYscHDq1"
   },
   "outputs": [],
   "source": [
    "#@title \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImhJDxNxw7Uk"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/computer-vision-um/cv-um-2022/trabajo-final/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es7fW58WFzaQ"
   },
   "source": [
    "\n",
    "# Ejercicio 2.1: Face Detection\n",
    "\n",
    "Tal como se explica en la letra del obligatorio final en este ejercicio es necesario implementar desde cero una solucion para detección de caras. Se proveen datos de entrenamiento y es necesario implementar su propio algoritmo de sliding window para entrenar un clasificador. Todo el código necesario para comenzar a trabajar está provisto en este notebook.\n",
    "\n",
    "\n",
    "\\**En los ejercicios del trabajo final es posible utilizar funciones de librerias existentes o código sacado de internet. Siempre y cuando **no se usen para resolver explicitamente lo que pide el ejercicio** y al código sacado de interenet le agreguen el link en comentarios de donde fue sacado ese código.*\n",
    "\n",
    "**Es necesario implemntar su propio algoritmo de sliding window**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiKqFLGiG_GF"
   },
   "source": [
    "##### Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R0bQDoWFlN8"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "import os\n",
    "import sklearn \n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from evaluation import evaluate_detector, precision_and_recall, interpolated_average_precision\n",
    "import sys\n",
    "from image_utils import non_max_suppression\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykAG8DVaHTBW"
   },
   "source": [
    "### Funciones Provistas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL0t1grWIgR_"
   },
   "source": [
    "#### Feature Extractors \n",
    "\n",
    "Para resolver el ejercicio van a tener que implementar las funciones `extract_hog_features` y `extract_lbp_features` pueden utilizar librerias como `skimage` o implementar ustedes mismos los descriptores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkDuSqQJIEUN"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractors(Enum):\n",
    "\t\tMiniImage = 1\n",
    "\t\tHOG = 2\n",
    "\t\tLBP = 3\n",
    "\n",
    "def extract_features(method, image):\n",
    "\t'''Switch between Feature extraction Methods'''\n",
    "\n",
    "\timage_representation = []\n",
    "\n",
    "\tif method == FeatureExtractors.MiniImage:\n",
    "\t\timage_representation = extract_mini_image_features(image)\n",
    "\telif method == FeatureExtractors.HOG:\n",
    "\t\timage_representation = extract_hog_features(image)\n",
    "\telif method == FeatureExtractors.LBP:\n",
    "\t\timage_representation = extract_lbp_features(image)\t\n",
    "\t\n",
    "\treturn image_representation\n",
    "\n",
    "def extract_mini_image_features(image,resize_size=(64,64)):\n",
    "    shape = image.shape\n",
    "    if len(shape) > 2:\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(image,resize_size)\n",
    "    image_representation = resized_image.reshape(resize_size[0]*resize_size[1])\n",
    "    return image_representation\n",
    "  \n",
    "def extract_lbp_features(img):\n",
    "  return []\n",
    "\n",
    "def extract_hog_features(img):\n",
    "  return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lPvlmEnIs2n"
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX-Tpj9JI1qt"
   },
   "outputs": [],
   "source": [
    "def load_training_data(training_positive_dir,trainign_negative_dir,feature_extractor=FeatureExtractors.MiniImage,target_size=[128,128], n_samples=2500):\n",
    "    ''' Function for loading loading training data from positive and negative examples\n",
    "    '''\n",
    "    positive_img_files = sorted(glob(training_positive_dir + '/*'))\n",
    "    negative_img_files = sorted(glob(trainign_negative_dir + '/*'))\n",
    "    #comment this line for loading all data\n",
    "    positive_img_files = positive_img_files[:n_samples]\n",
    "    negative_img_files = negative_img_files[:n_samples]\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    print('##Loading {} positive face images'.format(len(positive_img_files)))\n",
    "    for img in tqdm(positive_img_files, total=len(positive_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(1)\n",
    "    \n",
    "    print('##Loading {} negative face images'.format(len(negative_img_files)))\n",
    "    for img in tqdm(negative_img_files,total=len(negative_img_files)):\n",
    "        image = cv2.imread(img)[...,::-1]\n",
    "        image = cv2.resize(image, target_size, cv2.INTER_AREA)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(0)   \n",
    "    \n",
    "    training_data = np.asarray(training_data)\n",
    "    training_labels = np.asarray(training_labels)\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_validation_data(validation_data_dir):\n",
    "\n",
    "    validation_image_files = sorted(glob(validation_data_dir + '/*.jpg'))\n",
    "    val_images = []\n",
    "    validation_annotations= pd.read_pickle(os.path.join(validation_data_dir,'validation_bbox.pickle'))\n",
    "    print(validation_annotations.shape)\n",
    "    validation_bboxes = []\n",
    "    for img_file in tqdm(validation_image_files,total=len(validation_image_files)):\n",
    "        image = cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        val_images.append(image)\n",
    "        image_name = os.path.basename(img_file)\n",
    "        bbox_info = validation_annotations.loc[validation_annotations[\"image_id\"]==image_name]\n",
    "        bbox = np.array([bbox_info['x_left'].values[0],bbox_info['y_top'].values[0],bbox_info['x_left'].values[0]+bbox_info['width'].values[0],bbox_info['y_top'].values[0]+bbox_info['height'].values[0]])\n",
    "        validation_bboxes.append(bbox)\n",
    "        \n",
    "    return val_images, validation_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqb_HSDvJL9m"
   },
   "source": [
    "#### Sliding Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf1Ic0xYJghW"
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, window_size, scale, stride):\n",
    "    [image_rows, image_cols] = image.shape;\n",
    "    window_rows = window_size[0];\n",
    "    window_cols = window_size[1];\n",
    "\n",
    "    patches = np.zeros((window_rows, window_cols,5));\n",
    "    bbox_locations = np.zeros((5,4))\n",
    "    r = np.random.randint(0,image_rows-window_rows,5); # Sample top left position\n",
    "    c = np.random.randint(0,image_cols-window_cols,5);\n",
    "    for i in range(0,5):\n",
    "        patches[:,:,i] = image[r[i]:r[i]+window_rows, c[i]:c[i]+window_cols];\n",
    "        bbox_locations[i,:] = [r[i],c[i],window_rows,window_cols]; # top-left y,x, height, width\n",
    "\n",
    "\n",
    "    return patches, bbox_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvgD3gp8LK2l"
   },
   "source": [
    "##### Metodos Auxiliares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7dqYztRLcpg"
   },
   "outputs": [],
   "source": [
    "def show_image_with_bbox(image,bboxes,gt_bbox,draw_GT=True):\n",
    "    if draw_GT: \n",
    "        cv2.rectangle(image, (gt_bbox[0],gt_bbox[1]), (gt_bbox[2],gt_bbox[3]), (0, 0, 255), 2)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        if len(bbox) == 4:   \n",
    "            top_left = (int(bbox[0]),int(bbox[1]))\n",
    "            bottom_right = (int(bbox[0])+ int(bbox[2]),int(bbox[1])+int(bbox[3]))\n",
    "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ3ZbWKKFlOD"
   },
   "source": [
    "### Ubicación de los datos ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-zVz-OIFlOF"
   },
   "outputs": [],
   "source": [
    "data_dir='./'\n",
    "face_detection_dir = os.path.join(data_dir, 'face_detection')\n",
    "training_faces_dir = os.path.join(face_detection_dir,'cropped_faces')\n",
    "negative_examples_training_dir = os.path.join(face_detection_dir,'negative_data')\n",
    "validation_faces_dir = os.path.join(face_detection_dir,'validation')\n",
    "validation_raw_faces_dir = os.path.join(face_detection_dir,'val_raw_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU7x6hD5LUC7"
   },
   "source": [
    "## Entrenar Modelo y Face Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuo0icz8FlOG"
   },
   "source": [
    "### Cargar Datos de Entrenamiento ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtE_MyFWFlOH"
   },
   "outputs": [],
   "source": [
    "#Modify data_loader.py to load more training data\n",
    "training_data, trainig_labels = load_training_data(training_faces_dir,negative_examples_training_dir, FeatureExtractors.MiniImage)\n",
    "# You can save traninig_data and labels on nunmpy files to avoid processing data every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE3nYFzmFlOI"
   },
   "source": [
    "### Load Validation Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N00jneNFlOI"
   },
   "outputs": [],
   "source": [
    "validation_data = load_validation_data(validation_faces_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7poHeRhFlOI"
   },
   "source": [
    "### Entrenar un clasificador utilizando los datos de entrenamiento ## \n",
    "1. Una vez los datos de entrenamiento han sido cargados es necesario entrenar su propio clasificador \n",
    "2. Como solución inicial se utiliza un clasificador KNN pero para tener mejores resultados es posible entrenar un SVM. Se puede utilizar el [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) de sklearn a modo de ejemplo. Tambiés es posible utilizar la libreria [liblinear](https://pypi.org/project/liblinear-official/)\n",
    "3. Entrenar su clasificador y guardarlo en el archivo `face_detector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSpg1nVpFlOK"
   },
   "outputs": [],
   "source": [
    "knn_classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIQwPwWQFlOM"
   },
   "outputs": [],
   "source": [
    "knn_classifier.fit(training_data,trainig_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s2k3ufmFlOQ"
   },
   "source": [
    "#### Guardar el modelo entrenado ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fRvvaBOFlOR"
   },
   "outputs": [],
   "source": [
    "pickle.dump(knn_classifier,open('./face_detector', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWUPszrDFlOS"
   },
   "source": [
    "#### Cargar el Modelo entrenado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVizcW8qFlOS"
   },
   "outputs": [],
   "source": [
    "classifier = pickle.load(open('./face_detector','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JLCn_hOFlOS"
   },
   "source": [
    "### Sliding Window y Resultados ###\n",
    "En esta sección vamos a visualizar los resultados de la detección para ello hay que seguir los siguientes pasos:\n",
    "* Aplicar el algoritmo de sliding window a cada imagen.\n",
    "* Extraer features de cada parche y clasificar todos los partches de la imagen.\n",
    "* Applicar nonmax suppersion y elegir los parches finales con probabilidad mayor a 0.5 \n",
    "* Visualizar los resultados con la función provista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ceFJnT3FlOT"
   },
   "outputs": [],
   "source": [
    "window_size = (128, 128)\n",
    "scale = 1\n",
    "predictions = []\n",
    "threshold_p = 0.5\n",
    "overlap_threshold = 0.5\n",
    "validation_data, validation_bboxes = load_validation_data(validation_faces_dir)\n",
    "sample_images = 20\n",
    "stride = 32\n",
    "for img, gt_bbox in zip(validation_data[:sample_images],validation_bboxes[:sample_images]):\n",
    "    #print(img.shape)\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    patches, bbox_locations = sliding_window(gray_image,window_size,scale,stride)\n",
    "    print(patches.shape)\n",
    "    \n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.MiniImage, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    ## Get prediction label for each sliding window patch\n",
    "    if patches_feature_representation.shape[0] > 0:\n",
    "        scores = classifier.predict_proba(patches_feature_representation)\n",
    "        ## Positive Face Probabilities\n",
    "        face_probabilities = scores[:,1]\n",
    "        face_bboxes = bbox_locations[face_probabilities>threshold_p]\n",
    "        face_bboxes_probabilites = face_probabilities[face_probabilities>threshold_p]\n",
    "        # Do non max suppression and select strongest probability box\n",
    "        [selected_bbox, selected_score] = non_max_suppression(face_bboxes,face_bboxes_probabilites,0.3)\n",
    "    else:\n",
    "        selected_bbox = []\n",
    "    show_image_with_bbox(img, selected_bbox,gt_bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTKhHhrcFlOV"
   },
   "source": [
    "### Evaluate Detector ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_uqzQvfFlOW"
   },
   "outputs": [],
   "source": [
    "total_true_positives = []\n",
    "total_real_positives = []\n",
    "total_positive_predictions = []\n",
    "window_size = [64, 64]\n",
    "validation_data, validation_bboxes = load_validation_data(validation_faces_dir)\n",
    "k = 0\n",
    "stride = 8\n",
    "for img, gt_bbox in zip(validation_data,validation_bboxes):\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    patches, bbox_locations = sliding_window(gray_image,window_size,scale,stride)\n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.MiniImage, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    ## Get score for each sliding window patch\n",
    "    scores = classifier.predict_proba(patches_feature_representation)\n",
    "    ## Positive Face Probabilities\n",
    "    face_probabilities = scores[:,1]\n",
    "    ## liblinbear prediction\n",
    "    #[labels, acc, prob] = predict([],patches_feature_representation, clasifier)\n",
    "    #face_probabilities = np.asarray(prob)\n",
    "    #face_probabilities = face_probabilities.T[0]\n",
    "\n",
    "    [ detected_true_positives, image_real_positives, detected_faces ] = evaluate_detector( bbox_locations, face_probabilities);\n",
    "    total_true_positives.append(detected_true_positives)\n",
    "    total_real_positives.append(image_real_positives)\n",
    "    total_positive_predictions.append(detected_faces)\n",
    "        \n",
    "total_true_positives = np.asarray(total_true_positives)\n",
    "total_real_positives = np.asarray(total_real_positives)\n",
    "total_positive_predictions = np.asarray(total_positive_predictions)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwPNwJ3HxfwX"
   },
   "source": [
    "### Evaluation Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhxvUzlqFlOZ"
   },
   "outputs": [],
   "source": [
    "precision, recall = precision_and_recall(total_true_positives, total_real_positives,total_positive_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVy7HDnxFlOa"
   },
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0,1.1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw74Qp4VFlOc"
   },
   "outputs": [],
   "source": [
    "ap = interpolated_average_precision(recall,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBVw-MBqFlOc"
   },
   "outputs": [],
   "source": [
    "print('Detection Average Precision is {}'.format(ap))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Face Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
